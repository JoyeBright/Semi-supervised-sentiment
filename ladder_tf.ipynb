{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ladder_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPddFL8NwynAOv3qaxBBvS+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyeBright/Semi-supervised-sentiment/blob/ImageLadder/ladder_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxVe3Mf9XDfD",
        "colab_type": "text"
      },
      "source": [
        "**Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOz8gBNFi06d",
        "colab_type": "code",
        "outputId": "46453f54-b753-47e7-e094-fdd78cc083f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzQH4vZ3XZiy",
        "colab_type": "code",
        "outputId": "2421563f-9040-4149-ef10-6335b9331b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "%run /content/ladder_input.ipynb\n",
        "# Downgrade since colab installed tensorflow version 2.2.0 and it does not support placeholder as well\n",
        "!pip install tensorflow==1.1\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.1 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.18.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.1) (46.3.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp0CAg2MVUme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwwlqLRRVzmB",
        "colab_type": "text"
      },
      "source": [
        "### **Tensorflow Initilization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF9Emi4JmUjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_sizes = [784, 1000, 500, 250, 250, 250, 10]\n",
        "\n",
        "L = len(layer_sizes) - 1  # number of layers\n",
        "\n",
        "num_examples = 60000 # a training set of 60,000 examples\n",
        "num_epochs = 150\n",
        "num_labeled = 100\n",
        "\n",
        "started_learning_rate = 0.02\n",
        "\n",
        "decay_after = 15\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "num_iter = (num_examples/batch_size) * num_epochs\n",
        "\n",
        "inputs = tf.placeholder(tf.float32, shape=(None, layer_sizes[0]))\n",
        "outputs = tf.placeholder(tf.float32)\n",
        "\n",
        "def bi(inits, size, name):\n",
        "    return tf.Variable(inits * tf.ones([size]), name=name)\n",
        "\n",
        "def wi(shape, name):\n",
        "    return tf.Variable(tf.random_normal(shape, name=name)) / math.sqrt(shape[0])\n",
        "\n",
        "shapes = zip(layer_sizes[:-1], layer_sizes[1:])  # shapes of linear layers\n",
        "\n",
        "weights = {'W': [wi(s, \"W\") for s in shapes],  # Encoder weights\n",
        "                 'V': [wi(s[::-1], \"V\") for s in shapes],  # Decoder weights\n",
        "                 # batch normalization parameter to shift the normalized value\n",
        "                 'beta': [bi(0.0, layer_sizes[l+1], \"beta\") for l in range(L)],\n",
        "                 # batch normalization parameter to scale the normalized value\n",
        "                 'gamma': [bi(1.0, layer_sizes[l+1], \"beta\") for l in range(L)]}\n",
        "\n",
        "noise_std = 0.3  # scaling factor for noise used in corrupted encoder\n",
        "\n",
        "# hyperparameters that denote the importance of each layer\n",
        "denoising_cost = [1000.0, 10.0, 0.10, 0.10, 0.10, 0.10, 0.10]\n",
        "\n",
        "join = lambda l, u: tf.concat([l, u], 0)\n",
        "labeled = lambda x: tf.slice(x, [0, 0], [batch_size, -1]) if x is not None else x\n",
        "unlabeled = lambda x: tf.slice(x, [batch_size, 0], [-1, -1]) if x is not None else x\n",
        "split_lu = lambda x: (labeled(x), unlabeled(x))\n",
        "\n",
        "training = tf.placeholder(tf.bool)\n",
        "\n",
        "ewma = tf.train.ExponentialMovingAverage(decay=0.99)  # to calculate the moving averages of mean and variance\n",
        "bn_assigns = []  # this list stores the updates to be made to average mean and variance\n",
        "\n",
        "\n",
        "def batch_normalization(batch, mean=None, var=None):\n",
        "    if mean is None or var is None:\n",
        "        mean, var = tf.nn.moments(batch, axes=[0])\n",
        "    return (batch - mean) / tf.sqrt(var + tf.constant(1e-10))\n",
        "\n",
        "# average mean and variance of all layers\n",
        "running_mean = [tf.Variable(tf.constant(0.0, shape=[l]), trainable=False) for l in layer_sizes[1:]]\n",
        "running_var = [tf.Variable(tf.constant(1.0, shape=[l]), trainable=False) for l in layer_sizes[1:]]\n",
        "\n",
        "\n",
        "def update_batch_normalization(batch, l):\n",
        "    \"batch normalize + update average mean and variance of layer l\"\n",
        "    mean, var = tf.nn.moments(batch, axes=[0])\n",
        "    assign_mean = running_mean[l-1].assign(mean)\n",
        "    assign_var = running_var[l-1].assign(var)\n",
        "    bn_assigns.append(ewma.apply([running_mean[l-1], running_var[l-1]]))\n",
        "    with tf.control_dependencies([assign_mean, assign_var]):\n",
        "        return (batch - mean) / tf.sqrt(var + 1e-10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFWbQlSVdYK",
        "colab_type": "text"
      },
      "source": [
        "## **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-XNeTiYVcLS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ae1f75c8-3d54-4151-cdf4-6f3435823ba2"
      },
      "source": [
        "def encoder(inputs, noise_std):\n",
        "    h = inputs + tf.random_normal(tf.shape(inputs)) * noise_std  # add noise to input\n",
        "    d = {}  # to store the pre-activation, activation, mean and variance for each layer\n",
        "    # The data for labeled and unlabeled examples are stored separately\n",
        "    d['labeled'] = {'z': {}, 'm': {}, 'v': {}, 'h': {}}\n",
        "    d['unlabeled'] = {'z': {}, 'm': {}, 'v': {}, 'h': {}}\n",
        "    d['labeled']['z'][0], d['unlabeled']['z'][0] = split_lu(h)\n",
        "    for l in range(1, L+1):\n",
        "        print (\"Layer \", l, \": \", layer_sizes[l-1], \" -> \", layer_sizes[l])\n",
        "        d['labeled']['h'][l-1], d['unlabeled']['h'][l-1] = split_lu(h)\n",
        "        z_pre = tf.matmul(h, weights['W'][l-1])  # pre-activation\n",
        "        z_pre_l, z_pre_u = split_lu(z_pre)  # split labeled and unlabeled examples\n",
        "\n",
        "        m, v = tf.nn.moments(z_pre_u, axes=[0])\n",
        "\n",
        "        # if training:\n",
        "        def training_batch_norm():\n",
        "            # Training batch normalization\n",
        "            # batch normalization for labeled and unlabeled examples is performed separately\n",
        "            if noise_std > 0:\n",
        "                # Corrupted encoder\n",
        "                # batch normalization + noise\n",
        "                z = join(batch_normalization(z_pre_l), batch_normalization(z_pre_u, m, v))\n",
        "                z += tf.random_normal(tf.shape(z_pre)) * noise_std\n",
        "            else:\n",
        "                # Clean encoder\n",
        "                # batch normalization + update the average mean and variance using batch mean and variance of labeled examples\n",
        "                z = join(update_batch_normalization(z_pre_l, l), batch_normalization(z_pre_u, m, v))\n",
        "            return z\n",
        "\n",
        "        # else:\n",
        "        def eval_batch_norm():\n",
        "            # Evaluation batch normalization\n",
        "            # obtain average mean and variance and use it to normalize the batch\n",
        "            mean = ewma.average(running_mean[l-1])\n",
        "            var = ewma.average(running_var[l-1])\n",
        "            z = batch_normalization(z_pre, mean, var)\n",
        "            # Instead of the above statement, the use of the following 2 statements containing a typo\n",
        "            # consistently produces a 0.2% higher accuracy for unclear reasons.\n",
        "            # m_l, v_l = tf.nn.moments(z_pre_l, axes=[0])\n",
        "            # z = join(batch_normalization(z_pre_l, m_l, mean, var), batch_normalization(z_pre_u, mean, var))\n",
        "            return z\n",
        "\n",
        "        # perform batch normalization according to value of boolean \"training\" placeholder:\n",
        "        z = tf.cond(training, training_batch_norm, eval_batch_norm)\n",
        "\n",
        "        if l == L:\n",
        "            # use softmax activation in output layer\n",
        "            h = tf.nn.softmax(weights['gamma'][l-1] * (z + weights[\"beta\"][l-1]))\n",
        "        else:\n",
        "            # use ReLU activation in hidden layers\n",
        "            h = tf.nn.relu(z + weights[\"beta\"][l-1])\n",
        "        d['labeled']['z'][l], d['unlabeled']['z'][l] = split_lu(z)\n",
        "        d['unlabeled']['m'][l], d['unlabeled']['v'][l] = m, v  # save mean and variance of unlabeled examples for decoding\n",
        "    d['labeled']['h'][l], d['unlabeled']['h'][l] = split_lu(h)\n",
        "    return h, d\n",
        "\n",
        "print(\"=== Corrupted Encoder ===\")\n",
        "y_c, corr = encoder(inputs, noise_std)\n",
        "\n",
        "print(\"=== Clean Encoder ===\")\n",
        "y, clean = encoder(inputs, 0.0)  # 0.0 -> do not add noise\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Corrupted Encoder ===\n",
            "Layer  1 :  784  ->  1000\n",
            "Layer  2 :  1000  ->  500\n",
            "Layer  3 :  500  ->  250\n",
            "Layer  4 :  250  ->  250\n",
            "Layer  5 :  250  ->  250\n",
            "Layer  6 :  250  ->  10\n",
            "=== Clean Encoder ===\n",
            "Layer  1 :  784  ->  1000\n",
            "Layer  2 :  1000  ->  500\n",
            "Layer  3 :  500  ->  250\n",
            "Layer  4 :  250  ->  250\n",
            "Layer  5 :  250  ->  250\n",
            "Layer  6 :  250  ->  10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}