{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PiEnsembling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nMiiF7AmKEQf"
      ],
      "authorship_tag": "ABX9TyMVfE1Xk3z6nPAIB9XJu6Fq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyeBright/Semi-supervised-sentiment/blob/PiEnsembling/PiEnsembling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjLTzYCI4e5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np \n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMiiF7AmKEQf",
        "colab_type": "text"
      },
      "source": [
        "## **Some Necessary Imports and Modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdxzNTxQIx7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.layers import base\n",
        "from tensorflow.python.layers import utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "\n",
        "class _Conv(base.Layer):\n",
        "\n",
        "    def __init__(self, rank,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 strides=1,\n",
        "                 padding='valid',\n",
        "                 data_format='channels_last',\n",
        "                 dilation_rate=1,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=init_ops.zeros_initializer(),\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 trainable=True,\n",
        "                 weight_norm=True,\n",
        "                 mean_only_batch_norm=True,\n",
        "                 mean_only_batch_norm_momentum=0.99,\n",
        "                 name=None,\n",
        "                 **kwargs):\n",
        "        super(_Conv, self).__init__(trainable=trainable, name=name,\n",
        "                                    activity_regularizer=activity_regularizer,\n",
        "                                    **kwargs)\n",
        "        self.rank = rank\n",
        "        self.filters = filters\n",
        "        self.kernel_size = utils.normalize_tuple(\n",
        "            kernel_size, rank, 'kernel_size')\n",
        "        self.strides = utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.padding = utils.normalize_padding(padding)\n",
        "        self.data_format = utils.normalize_data_format(data_format)\n",
        "        self.dilation_rate = utils.normalize_tuple(\n",
        "            dilation_rate, rank, 'dilation_rate')\n",
        "        self.activation = activation\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.bias_regularizer = bias_regularizer\n",
        "        self.kernel_constraint = kernel_constraint\n",
        "        self.bias_constraint = bias_constraint\n",
        "        self.input_spec = base.InputSpec(ndim=self.rank + 2)\n",
        "        self.weight_norm = weight_norm\n",
        "        self.mean_only_batch_norm = mean_only_batch_norm\n",
        "        self.mean_only_batch_norm_momentum = mean_only_batch_norm_momentum\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis].value is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis].value\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        self.kernel = self.add_variable(name='kernel',\n",
        "                                        shape=kernel_shape,\n",
        "                                        initializer=self.kernel_initializer,\n",
        "                                        regularizer=self.kernel_regularizer,\n",
        "                                        constraint=self.kernel_constraint,\n",
        "                                        trainable=True,\n",
        "                                        dtype=self.dtype)\n",
        "\n",
        "        if self.weight_norm:\n",
        "            self.V = self.add_variable(name='V_weight_norm',\n",
        "                                       shape=kernel_shape,\n",
        "                                       dtype=tf.float32,\n",
        "                                       initializer=tf.random_normal_initializer(\n",
        "                                           0, 0.05),\n",
        "                                       trainable=True)\n",
        "            self.g = self.add_variable(name='g_weight_norm',\n",
        "                                       shape=(self.filters,),\n",
        "                                       initializer=init_ops.ones_initializer(),\n",
        "                                       dtype=self.dtype,\n",
        "                                       trainable=True)\n",
        "        if self.mean_only_batch_norm:\n",
        "            self.batch_norm_running_average = []\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_variable(name='bias',\n",
        "                                          shape=(self.filters,),\n",
        "                                          initializer=self.bias_initializer,\n",
        "                                          regularizer=self.bias_regularizer,\n",
        "                                          constraint=self.bias_constraint,\n",
        "                                          trainable=True,\n",
        "                                          dtype=self.dtype)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.input_spec = base.InputSpec(ndim=self.rank + 2,\n",
        "                                         axes={channel_axis: input_dim})\n",
        "        self._convolution_op = nn_ops.Convolution(\n",
        "            input_shape,\n",
        "            filter_shape=self.kernel.get_shape(),\n",
        "            dilation_rate=self.dilation_rate,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding.upper(),\n",
        "            data_format=utils.convert_data_format(self.data_format,\n",
        "                                                  self.rank + 2))\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        if self.weight_norm:\n",
        "            self.kernel = tf.reshape(\n",
        "                self.g, [1, 1, 1, self.filters])*tf.nn.l2_normalize(self.V, [0, 1, 2])\n",
        "\n",
        "        outputs = self._convolution_op(inputs, self.kernel)\n",
        "\n",
        "        if self.mean_only_batch_norm:\n",
        "            mean = tf.reduce_mean(outputs, reduction_indices=0)\n",
        "            if training:\n",
        "                # If first iteration\n",
        "                if self.batch_norm_running_average == []:\n",
        "                    self.batch_norm_running_average = mean\n",
        "                else:\n",
        "                    self.batch_norm_running_average = self.batch_norm_running_average * \\\n",
        "                        self.mean_only_batch_norm_momentum + mean * \\\n",
        "                        (1-self.mean_only_batch_norm_momentum)\n",
        "                outputs = outputs - mean\n",
        "            else:\n",
        "                outputs = outputs - self.batch_norm_running_average\n",
        "\n",
        "        if self.use_bias:\n",
        "            if self.data_format == 'channels_first':\n",
        "                if self.rank == 1:\n",
        "                    # nn.bias_add does not accept a 1D input tensor.\n",
        "                    bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "                    outputs += bias\n",
        "                if self.rank == 2:\n",
        "                    outputs = nn.bias_add(\n",
        "                        outputs, self.bias, data_format='NCHW')\n",
        "                if self.rank == 3:\n",
        "                    # As of Mar 2017, direct addition is significantly slower than\n",
        "                    # bias_add when computing gradients. To use bias_add, we collapse Z\n",
        "                    # and Y into a single dimension to obtain a 4D input tensor.\n",
        "                    outputs_shape = outputs.shape.as_list()\n",
        "                    if outputs_shape[0] is None:\n",
        "                        outputs_shape[0] = -1\n",
        "                    outputs_4d = array_ops.reshape(outputs,\n",
        "                                                   [outputs_shape[0], outputs_shape[1],\n",
        "                                                    outputs_shape[2] *\n",
        "                                                    outputs_shape[3],\n",
        "                                                    outputs_shape[4]])\n",
        "                    outputs_4d = nn.bias_add(\n",
        "                        outputs_4d, self.bias, data_format='NCHW')\n",
        "                    outputs = array_ops.reshape(outputs_4d, outputs_shape)\n",
        "            else:\n",
        "                outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
        "        if self.data_format == 'channels_last':\n",
        "            space = input_shape[1:-1]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding=self.padding,\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            return tensor_shape.TensorShape([input_shape[0]] + new_space +\n",
        "                                            [self.filters])\n",
        "        else:\n",
        "            space = input_shape[2:]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding=self.padding,\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            return tensor_shape.TensorShape([input_shape[0], self.filters] +\n",
        "                                            new_space)\n",
        "\n",
        "\n",
        "@tf_export('layers.Conv2D')\n",
        "class Conv2D(_Conv):\n",
        "\n",
        "    def __init__(self, filters,\n",
        "                 kernel_size,\n",
        "                 strides=(1, 1),\n",
        "                 padding='valid',\n",
        "                 data_format='channels_last',\n",
        "                 dilation_rate=(1, 1),\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=init_ops.zeros_initializer(),\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 trainable=True,\n",
        "                 weight_norm=True,\n",
        "                 mean_only_batch_norm=True,\n",
        "                 name=None,\n",
        "                 **kwargs):\n",
        "        super(Conv2D, self).__init__(\n",
        "            rank=2,\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding=padding,\n",
        "            data_format=data_format,\n",
        "            dilation_rate=dilation_rate,\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            trainable=trainable,\n",
        "            weight_norm=weight_norm,\n",
        "            mean_only_batch_norm=mean_only_batch_norm,\n",
        "            name=name, **kwargs)\n",
        "\n",
        "\n",
        "@tf_export('layers.conv2d')\n",
        "def conv2d(inputs,\n",
        "           filters,\n",
        "           kernel_size,\n",
        "           strides=(1, 1),\n",
        "           padding='valid',\n",
        "           data_format='channels_last',\n",
        "           dilation_rate=(1, 1),\n",
        "           activation=None,\n",
        "           use_bias=True,\n",
        "           kernel_initializer=None,\n",
        "           bias_initializer=init_ops.zeros_initializer(),\n",
        "           kernel_regularizer=None,\n",
        "           bias_regularizer=None,\n",
        "           activity_regularizer=None,\n",
        "           kernel_constraint=None,\n",
        "           bias_constraint=None,\n",
        "           weight_norm=True,\n",
        "           mean_only_batch_norm=True,\n",
        "           trainable=True,\n",
        "           name=None,\n",
        "           reuse=None):\n",
        "  \n",
        "    layer = Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        data_format=data_format,\n",
        "        dilation_rate=dilation_rate,\n",
        "        activation=activation,\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        bias_initializer=bias_initializer,\n",
        "        kernel_regularizer=kernel_regularizer,\n",
        "        bias_regularizer=bias_regularizer,\n",
        "        activity_regularizer=activity_regularizer,\n",
        "        kernel_constraint=kernel_constraint,\n",
        "        bias_constraint=bias_constraint,\n",
        "        trainable=trainable,\n",
        "        name=name,\n",
        "        dtype=inputs.dtype.base_dtype,\n",
        "        _reuse=reuse,\n",
        "        _scope=name)\n",
        "    return layer.apply(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ITxur8JW0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import six\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.layers import base\n",
        "from tensorflow.python.layers import utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.ops import standard_ops\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "\n",
        "@tf_export('layers.Dense')\n",
        "class Dense(base.Layer):\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=init_ops.zeros_initializer(),\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 trainable=True,\n",
        "                 weight_norm=True,\n",
        "                 mean_only_batch_norm=True,\n",
        "                 mean_only_batch_norm_momentum=0.99,\n",
        "                 name=None,\n",
        "                 **kwargs):\n",
        "        super(Dense, self).__init__(trainable=trainable, name=name,\n",
        "                                    activity_regularizer=activity_regularizer,\n",
        "                                    **kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.bias_regularizer = bias_regularizer\n",
        "        self.kernel_constraint = kernel_constraint\n",
        "        self.bias_constraint = bias_constraint\n",
        "        self.input_spec = base.InputSpec(min_ndim=2)\n",
        "        self.weight_norm = weight_norm\n",
        "        self.mean_only_batch_norm = mean_only_batch_norm\n",
        "        self.mean_only_batch_norm_momentum = mean_only_batch_norm_momentum\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        if input_shape[-1].value is None:\n",
        "            raise ValueError('The last dimension of the inputs to `Dense` '\n",
        "                             'should be defined. Found `None`.')\n",
        "        self.input_spec = base.InputSpec(min_ndim=2,\n",
        "                                         axes={-1: input_shape[-1].value})\n",
        "        self.kernel = self.add_variable('kernel',\n",
        "                                        shape=[\n",
        "                                            input_shape[-1].value, self.units],\n",
        "                                        initializer=self.kernel_initializer,\n",
        "                                        regularizer=self.kernel_regularizer,\n",
        "                                        constraint=self.kernel_constraint,\n",
        "                                        dtype=self.dtype,\n",
        "                                        trainable=True)\n",
        "\n",
        "        if self.weight_norm:\n",
        "            self.V = self.add_variable(name='V_weight_norm',\n",
        "                                       shape=[\n",
        "                                            input_shape[-1].value, self.units],\n",
        "                                       dtype=tf.float32,\n",
        "                                       initializer=tf.random_normal_initializer(\n",
        "                                           0, 0.05),\n",
        "                                       trainable=True)\n",
        "            self.g = self.add_variable(name='g_weight_norm',\n",
        "                                       shape=(self.units,),\n",
        "                                       initializer=init_ops.ones_initializer(),\n",
        "                                       dtype=self.dtype,\n",
        "                                       trainable=True)\n",
        "\n",
        "        if self.mean_only_batch_norm:\n",
        "            self.batch_norm_running_average = []\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_variable('bias',\n",
        "                                          shape=[self.units, ],\n",
        "                                          initializer=self.bias_initializer,\n",
        "                                          regularizer=self.bias_regularizer,\n",
        "                                          constraint=self.bias_constraint,\n",
        "                                          dtype=self.dtype,\n",
        "                                          trainable=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\n",
        "        shape = inputs.get_shape().as_list()\n",
        "\n",
        "        if self.weight_norm:\n",
        "            inputs = tf.matmul(inputs, self.V)\n",
        "            scaler = self.g/tf.sqrt(tf.reduce_sum(tf.square(self.V), [0]))\n",
        "            outputs = tf.reshape(scaler, [1, self.units])*inputs\n",
        "        else:\n",
        "            if len(shape) > 2:\n",
        "                # Broadcasting is required for the inputs.\n",
        "                outputs = standard_ops.tensordot(inputs, self.kernel, [[len(shape) - 1],\n",
        "                                                                       [0]])\n",
        "                # Reshape the output back to the original ndim of the input.\n",
        "                if not context.executing_eagerly():\n",
        "                    output_shape = shape[:-1] + [self.units]\n",
        "                    outputs.set_shape(output_shape)\n",
        "            else:\n",
        "                outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n",
        "\n",
        "\n",
        "        if self.mean_only_batch_norm:\n",
        "            mean = tf.reduce_mean(outputs, reduction_indices=0)\n",
        "            if training:\n",
        "                # If first iteration\n",
        "                if self.batch_norm_running_average == []:\n",
        "                    self.batch_norm_running_average = mean\n",
        "                else:\n",
        "                    self.batch_norm_running_average = self.batch_norm_running_average * \\\n",
        "                        self.mean_only_batch_norm_momentum + mean * \\\n",
        "                        (1-self.mean_only_batch_norm_momentum)\n",
        "                    outputs = outputs - mean\n",
        "            else:\n",
        "                outputs = outputs - self.batch_norm_running_average\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)  # pylint: disable=not-callable\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        input_shape = input_shape.with_rank_at_least(2)\n",
        "        if input_shape[-1].value is None:\n",
        "            raise ValueError(\n",
        "                'The innermost dimension of input_shape must be defined, but saw: %s'\n",
        "                % input_shape)\n",
        "        return input_shape[:-1].concatenate(self.units)\n",
        "\n",
        "\n",
        "@tf_export('layers.dense')\n",
        "def dense(\n",
        "        inputs, units,\n",
        "        activation=None,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=None,\n",
        "        bias_initializer=init_ops.zeros_initializer(),\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        weight_norm=True,\n",
        "        mean_only_batch_norm=True,\n",
        "        trainable=True,\n",
        "        name=None,\n",
        "        reuse=None):\n",
        "\n",
        "    layer = Dense(units,\n",
        "                  activation=activation,\n",
        "                  use_bias=use_bias,\n",
        "                  kernel_initializer=kernel_initializer,\n",
        "                  bias_initializer=bias_initializer,\n",
        "                  kernel_regularizer=kernel_regularizer,\n",
        "                  bias_regularizer=bias_regularizer,\n",
        "                  activity_regularizer=activity_regularizer,\n",
        "                  kernel_constraint=kernel_constraint,\n",
        "                  bias_constraint=bias_constraint,\n",
        "                  trainable=trainable,\n",
        "                  weight_norm=weight_norm,\n",
        "                  mean_only_batch_norm=mean_only_batch_norm,\n",
        "                  name=name,\n",
        "                  dtype=inputs.dtype.base_dtype,\n",
        "                  _scope=name,\n",
        "                  _reuse=reuse)\n",
        "    return layer.apply(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jAvBmXCKKFO",
        "colab_type": "text"
      },
      "source": [
        "**Loss of Pi Ensembling Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18vRXQE5J6IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pi_model_loss(X_train_labeled, y_train_labeled, X_train_unlabeled,\n",
        "                  pi_model, unsupervised_weight):\n",
        "\n",
        "    z_labeled = pi_model(X_train_labeled)\n",
        "    z_labeled_i = pi_model(X_train_labeled)\n",
        "\n",
        "    z_unlabeled = pi_model(X_train_unlabeled)\n",
        "    z_unlabeled_i = pi_model(X_train_unlabeled)\n",
        "\n",
        "    # Loss = Supervised loss + unsup loss of labeled sample + unsup loss unlabeled sample (Unsupervised Loss)\n",
        "    return tf.losses.softmax_cross_entropy(\n",
        "        y_train_labeled, z_labeled) + unsupervised_weight * (\n",
        "            tf.losses.mean_squared_error(z_labeled, z_labeled_i) +\n",
        "            tf.losses.mean_squared_error(z_unlabeled, z_unlabeled_i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QKLOS23KYRo",
        "colab_type": "text"
      },
      "source": [
        "## **Pi Gradient Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seKFoL2oKdS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pi_model_gradients(X_train_labeled, y_train_labeled, X_train_unlabeled,\n",
        "                       pi_model, unsupervised_weight):\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = pi_model_loss(X_train_labeled, y_train_labeled, X_train_unlabeled,\n",
        "                                   pi_model, unsupervised_weight)\n",
        "    return loss_value, tape.gradient(loss_value, pi_model.variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmH1PPemKiTe",
        "colab_type": "text"
      },
      "source": [
        "## **Ramp-up Function**\n",
        "NB: keep it slow in intial epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlYt5kXNKuvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ramp_up_function(epoch, epoch_with_max_rampup=80):\n",
        "\n",
        "    if epoch < epoch_with_max_rampup:\n",
        "        p = max(0.0, float(epoch)) / float(epoch_with_max_rampup)\n",
        "        p = 1.0 - p\n",
        "        return math.exp(-p*p*5.0)\n",
        "    else:\n",
        "        return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}