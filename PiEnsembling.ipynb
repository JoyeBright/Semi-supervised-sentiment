{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PiEnsembling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nMiiF7AmKEQf",
        "2QKLOS23KYRo",
        "3jp07fVoLVYs"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1197b01ceb9f463db3a49876ced36f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea61a0eaa07541c1a5e5f11e92685999",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5faf311b82d04dbb8d2d54e35041f084",
              "IPY_MODEL_5d4e1c2f3c7c4576a568912fbc367b97"
            ]
          }
        },
        "ea61a0eaa07541c1a5e5f11e92685999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5faf311b82d04dbb8d2d54e35041f084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4dc4c560c5d245ac806ef4d49f3b587f",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 300,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18e07c31fa714d9cb1dd367340f7dcf9"
          }
        },
        "5d4e1c2f3c7c4576a568912fbc367b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_348c8eeda4bf4252bb6cd288e35d91bc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/300 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_704ea2d76af04886953e0160229879a4"
          }
        },
        "4dc4c560c5d245ac806ef4d49f3b587f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18e07c31fa714d9cb1dd367340f7dcf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "348c8eeda4bf4252bb6cd288e35d91bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "704ea2d76af04886953e0160229879a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80eb3e6a517144cbbc1ab1666bfef08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_45602955c4904e2588d7d6564456b98f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_83c227f38a214ac6af042cd4b5adfd9a",
              "IPY_MODEL_b2df7d50ab53451595d4d80f73615039"
            ]
          }
        },
        "45602955c4904e2588d7d6564456b98f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83c227f38a214ac6af042cd4b5adfd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_404cc24a610447fa8f0e66cdd6fb0139",
            "_dom_classes": [],
            "description": " 98%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 40,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 39,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1ffacc387d7c4b2796c3c5c2b7b9da1e"
          }
        },
        "b2df7d50ab53451595d4d80f73615039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2ce562d92dc94732892d32f95d9afa6b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 39/40 [10:00&lt;00:15, 15.30s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7873bb4310c49288119e12edb74358f"
          }
        },
        "404cc24a610447fa8f0e66cdd6fb0139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1ffacc387d7c4b2796c3c5c2b7b9da1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ce562d92dc94732892d32f95d9afa6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7873bb4310c49288119e12edb74358f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyeBright/Semi-supervised-sentiment/blob/PiEnsembling/PiEnsembling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "87SZOhS9S1nu",
        "outputId": "c5ab304c-77da-4437-c0ae-808af1b627f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "!pip install tensorflow==1.13.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.29.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.18.4)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 32.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (47.1.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.1.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SjLTzYCI4e5a",
        "outputId": "98b66f1c-c2ee-43bb-8c79-48ba81a05e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "import math\n",
        "import numpy as np \n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nMiiF7AmKEQf"
      },
      "source": [
        "## **Some Necessary Imports and Modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KdxzNTxQIx7Y",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.layers import base\n",
        "from tensorflow.python.layers import utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "\n",
        "class _Conv(base.Layer):\n",
        "\n",
        "    def __init__(self, rank,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 strides=1,\n",
        "                 padding='valid',\n",
        "                 data_format='channels_last',\n",
        "                 dilation_rate=1,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=init_ops.zeros_initializer(),\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 trainable=True,\n",
        "                 weight_norm=True,\n",
        "                 mean_only_batch_norm=True,\n",
        "                 mean_only_batch_norm_momentum=0.99,\n",
        "                 name=None,\n",
        "                 **kwargs):\n",
        "        super(_Conv, self).__init__(trainable=trainable, name=name,\n",
        "                                    activity_regularizer=activity_regularizer,\n",
        "                                    **kwargs)\n",
        "        self.rank = rank\n",
        "        self.filters = filters\n",
        "        self.kernel_size = utils.normalize_tuple(\n",
        "            kernel_size, rank, 'kernel_size')\n",
        "        self.strides = utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.padding = utils.normalize_padding(padding)\n",
        "        self.data_format = utils.normalize_data_format(data_format)\n",
        "        self.dilation_rate = utils.normalize_tuple(\n",
        "            dilation_rate, rank, 'dilation_rate')\n",
        "        self.activation = activation\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.bias_regularizer = bias_regularizer\n",
        "        self.kernel_constraint = kernel_constraint\n",
        "        self.bias_constraint = bias_constraint\n",
        "        self.input_spec = base.InputSpec(ndim=self.rank + 2)\n",
        "        self.weight_norm = weight_norm\n",
        "        self.mean_only_batch_norm = mean_only_batch_norm\n",
        "        self.mean_only_batch_norm_momentum = mean_only_batch_norm_momentum\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis = 1\n",
        "        else:\n",
        "            channel_axis = -1\n",
        "        if input_shape[channel_axis].value is None:\n",
        "            raise ValueError('The channel dimension of the inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        input_dim = input_shape[channel_axis].value\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        self.kernel = self.add_variable(name='kernel',\n",
        "                                        shape=kernel_shape,\n",
        "                                        initializer=self.kernel_initializer,\n",
        "                                        regularizer=self.kernel_regularizer,\n",
        "                                        constraint=self.kernel_constraint,\n",
        "                                        trainable=True,\n",
        "                                        dtype=self.dtype)\n",
        "\n",
        "        if self.weight_norm:\n",
        "            self.V = self.add_variable(name='V_weight_norm',\n",
        "                                       shape=kernel_shape,\n",
        "                                       dtype=tf.float32,\n",
        "                                       initializer=tf.random_normal_initializer(\n",
        "                                           0, 0.05),\n",
        "                                       trainable=True)\n",
        "            self.g = self.add_variable(name='g_weight_norm',\n",
        "                                       shape=(self.filters,),\n",
        "                                       initializer=init_ops.ones_initializer(),\n",
        "                                       dtype=self.dtype,\n",
        "                                       trainable=True)\n",
        "        if self.mean_only_batch_norm:\n",
        "            self.batch_norm_running_average = []\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_variable(name='bias',\n",
        "                                          shape=(self.filters,),\n",
        "                                          initializer=self.bias_initializer,\n",
        "                                          regularizer=self.bias_regularizer,\n",
        "                                          constraint=self.bias_constraint,\n",
        "                                          trainable=True,\n",
        "                                          dtype=self.dtype)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.input_spec = base.InputSpec(ndim=self.rank + 2,\n",
        "                                         axes={channel_axis: input_dim})\n",
        "        self._convolution_op = nn_ops.Convolution(\n",
        "            input_shape,\n",
        "            filter_shape=self.kernel.get_shape(),\n",
        "            dilation_rate=self.dilation_rate,\n",
        "            strides=self.strides,\n",
        "            padding=self.padding.upper(),\n",
        "            data_format=utils.convert_data_format(self.data_format,\n",
        "                                                  self.rank + 2))\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        if self.weight_norm:\n",
        "            self.kernel = tf.reshape(\n",
        "                self.g, [1, 1, 1, self.filters])*tf.nn.l2_normalize(self.V, [0, 1, 2])\n",
        "\n",
        "        outputs = self._convolution_op(inputs, self.kernel)\n",
        "\n",
        "        if self.mean_only_batch_norm:\n",
        "            mean = tf.reduce_mean(outputs, reduction_indices=0)\n",
        "            if training:\n",
        "                # If first iteration\n",
        "                if self.batch_norm_running_average == []:\n",
        "                    self.batch_norm_running_average = mean\n",
        "                else:\n",
        "                    self.batch_norm_running_average = self.batch_norm_running_average * \\\n",
        "                        self.mean_only_batch_norm_momentum + mean * \\\n",
        "                        (1-self.mean_only_batch_norm_momentum)\n",
        "                outputs = outputs - mean\n",
        "            else:\n",
        "                outputs = outputs - self.batch_norm_running_average\n",
        "\n",
        "        if self.use_bias:\n",
        "            if self.data_format == 'channels_first':\n",
        "                if self.rank == 1:\n",
        "                    # nn.bias_add does not accept a 1D input tensor.\n",
        "                    bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "                    outputs += bias\n",
        "                if self.rank == 2:\n",
        "                    outputs = nn.bias_add(\n",
        "                        outputs, self.bias, data_format='NCHW')\n",
        "                if self.rank == 3:\n",
        "                    # As of Mar 2017, direct addition is significantly slower than\n",
        "                    # bias_add when computing gradients. To use bias_add, we collapse Z\n",
        "                    # and Y into a single dimension to obtain a 4D input tensor.\n",
        "                    outputs_shape = outputs.shape.as_list()\n",
        "                    if outputs_shape[0] is None:\n",
        "                        outputs_shape[0] = -1\n",
        "                    outputs_4d = array_ops.reshape(outputs,\n",
        "                                                   [outputs_shape[0], outputs_shape[1],\n",
        "                                                    outputs_shape[2] *\n",
        "                                                    outputs_shape[3],\n",
        "                                                    outputs_shape[4]])\n",
        "                    outputs_4d = nn.bias_add(\n",
        "                        outputs_4d, self.bias, data_format='NCHW')\n",
        "                    outputs = array_ops.reshape(outputs_4d, outputs_shape)\n",
        "            else:\n",
        "                outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
        "\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape).as_list()\n",
        "        if self.data_format == 'channels_last':\n",
        "            space = input_shape[1:-1]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding=self.padding,\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            return tensor_shape.TensorShape([input_shape[0]] + new_space +\n",
        "                                            [self.filters])\n",
        "        else:\n",
        "            space = input_shape[2:]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding=self.padding,\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            return tensor_shape.TensorShape([input_shape[0], self.filters] +\n",
        "                                            new_space)\n",
        "\n",
        "\n",
        "@tf_export('layers.Conv2D')\n",
        "class Conv2D(_Conv):\n",
        "\n",
        "    def __init__(self, filters,\n",
        "                 kernel_size,\n",
        "                 strides=(1, 1),\n",
        "                 padding='valid',\n",
        "                 data_format='channels_last',\n",
        "                 dilation_rate=(1, 1),\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=init_ops.zeros_initializer(),\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 trainable=True,\n",
        "                 weight_norm=True,\n",
        "                 mean_only_batch_norm=True,\n",
        "                 name=None,\n",
        "                 **kwargs):\n",
        "        super(Conv2D, self).__init__(\n",
        "            rank=2,\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding=padding,\n",
        "            data_format=data_format,\n",
        "            dilation_rate=dilation_rate,\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            trainable=trainable,\n",
        "            weight_norm=weight_norm,\n",
        "            mean_only_batch_norm=mean_only_batch_norm,\n",
        "            name=name, **kwargs)\n",
        "\n",
        "\n",
        "@tf_export('layers.conv2d')\n",
        "def conv2d(inputs,\n",
        "           filters,\n",
        "           kernel_size,\n",
        "           strides=(1, 1),\n",
        "           padding='valid',\n",
        "           data_format='channels_last',\n",
        "           dilation_rate=(1, 1),\n",
        "           activation=None,\n",
        "           use_bias=True,\n",
        "           kernel_initializer=None,\n",
        "           bias_initializer=init_ops.zeros_initializer(),\n",
        "           kernel_regularizer=None,\n",
        "           bias_regularizer=None,\n",
        "           activity_regularizer=None,\n",
        "           kernel_constraint=None,\n",
        "           bias_constraint=None,\n",
        "           weight_norm=True,\n",
        "           mean_only_batch_norm=True,\n",
        "           trainable=True,\n",
        "           name=None,\n",
        "           reuse=None):\n",
        "  \n",
        "    layer = Conv2D(\n",
        "        filters=filters,\n",
        "        kernel_size=kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        data_format=data_format,\n",
        "        dilation_rate=dilation_rate,\n",
        "        activation=activation,\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        bias_initializer=bias_initializer,\n",
        "        kernel_regularizer=kernel_regularizer,\n",
        "        bias_regularizer=bias_regularizer,\n",
        "        activity_regularizer=activity_regularizer,\n",
        "        kernel_constraint=kernel_constraint,\n",
        "        bias_constraint=bias_constraint,\n",
        "        trainable=trainable,\n",
        "        name=name,\n",
        "        dtype=inputs.dtype.base_dtype,\n",
        "        _reuse=reuse,\n",
        "        _scope=name)\n",
        "    return layer.apply(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W2ITxur8JW0h",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "import six\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.layers import base\n",
        "from tensorflow.python.layers import utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import init_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.ops import standard_ops\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "\n",
        "@tf_export('layers.Dense')\n",
        "class Dense(base.Layer):\n",
        "\n",
        "    def __init__(self, units,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer=None,\n",
        "                 bias_initializer=init_ops.zeros_initializer(),\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 trainable=True,\n",
        "                 weight_norm=True,\n",
        "                 mean_only_batch_norm=True,\n",
        "                 mean_only_batch_norm_momentum=0.99,\n",
        "                 name=None,\n",
        "                 **kwargs):\n",
        "        super(Dense, self).__init__(trainable=trainable, name=name,\n",
        "                                    activity_regularizer=activity_regularizer,\n",
        "                                    **kwargs)\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.use_bias = use_bias\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.bias_regularizer = bias_regularizer\n",
        "        self.kernel_constraint = kernel_constraint\n",
        "        self.bias_constraint = bias_constraint\n",
        "        self.input_spec = base.InputSpec(min_ndim=2)\n",
        "        self.weight_norm = weight_norm\n",
        "        self.mean_only_batch_norm = mean_only_batch_norm\n",
        "        self.mean_only_batch_norm_momentum = mean_only_batch_norm_momentum\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        if input_shape[-1].value is None:\n",
        "            raise ValueError('The last dimension of the inputs to `Dense` '\n",
        "                             'should be defined. Found `None`.')\n",
        "        self.input_spec = base.InputSpec(min_ndim=2,\n",
        "                                         axes={-1: input_shape[-1].value})\n",
        "        self.kernel = self.add_variable('kernel',\n",
        "                                        shape=[\n",
        "                                            input_shape[-1].value, self.units],\n",
        "                                        initializer=self.kernel_initializer,\n",
        "                                        regularizer=self.kernel_regularizer,\n",
        "                                        constraint=self.kernel_constraint,\n",
        "                                        dtype=self.dtype,\n",
        "                                        trainable=True)\n",
        "\n",
        "        if self.weight_norm:\n",
        "            self.V = self.add_variable(name='V_weight_norm',\n",
        "                                       shape=[\n",
        "                                            input_shape[-1].value, self.units],\n",
        "                                       dtype=tf.float32,\n",
        "                                       initializer=tf.random_normal_initializer(\n",
        "                                           0, 0.05),\n",
        "                                       trainable=True)\n",
        "            self.g = self.add_variable(name='g_weight_norm',\n",
        "                                       shape=(self.units,),\n",
        "                                       initializer=init_ops.ones_initializer(),\n",
        "                                       dtype=self.dtype,\n",
        "                                       trainable=True)\n",
        "\n",
        "        if self.mean_only_batch_norm:\n",
        "            self.batch_norm_running_average = []\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_variable('bias',\n",
        "                                          shape=[self.units, ],\n",
        "                                          initializer=self.bias_initializer,\n",
        "                                          regularizer=self.bias_regularizer,\n",
        "                                          constraint=self.bias_constraint,\n",
        "                                          dtype=self.dtype,\n",
        "                                          trainable=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\n",
        "        shape = inputs.get_shape().as_list()\n",
        "\n",
        "        if self.weight_norm:\n",
        "            inputs = tf.matmul(inputs, self.V)\n",
        "            scaler = self.g/tf.sqrt(tf.reduce_sum(tf.square(self.V), [0]))\n",
        "            outputs = tf.reshape(scaler, [1, self.units])*inputs\n",
        "        else:\n",
        "            if len(shape) > 2:\n",
        "                # Broadcasting is required for the inputs.\n",
        "                outputs = standard_ops.tensordot(inputs, self.kernel, [[len(shape) - 1],\n",
        "                                                                       [0]])\n",
        "                # Reshape the output back to the original ndim of the input.\n",
        "                if not context.executing_eagerly():\n",
        "                    output_shape = shape[:-1] + [self.units]\n",
        "                    outputs.set_shape(output_shape)\n",
        "            else:\n",
        "                outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n",
        "\n",
        "\n",
        "        if self.mean_only_batch_norm:\n",
        "            mean = tf.reduce_mean(outputs, reduction_indices=0)\n",
        "            if training:\n",
        "                # If first iteration\n",
        "                if self.batch_norm_running_average == []:\n",
        "                    self.batch_norm_running_average = mean\n",
        "                else:\n",
        "                    self.batch_norm_running_average = self.batch_norm_running_average * \\\n",
        "                        self.mean_only_batch_norm_momentum + mean * \\\n",
        "                        (1-self.mean_only_batch_norm_momentum)\n",
        "                    outputs = outputs - mean\n",
        "            else:\n",
        "                outputs = outputs - self.batch_norm_running_average\n",
        "\n",
        "        if self.use_bias:\n",
        "            outputs = nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)  # pylint: disable=not-callable\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        input_shape = input_shape.with_rank_at_least(2)\n",
        "        if input_shape[-1].value is None:\n",
        "            raise ValueError(\n",
        "                'The innermost dimension of input_shape must be defined, but saw: %s'\n",
        "                % input_shape)\n",
        "        return input_shape[:-1].concatenate(self.units)\n",
        "\n",
        "\n",
        "@tf_export('layers.dense')\n",
        "def dense(\n",
        "        inputs, units,\n",
        "        activation=None,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=None,\n",
        "        bias_initializer=init_ops.zeros_initializer(),\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        weight_norm=True,\n",
        "        mean_only_batch_norm=True,\n",
        "        trainable=True,\n",
        "        name=None,\n",
        "        reuse=None):\n",
        "\n",
        "    layer = Dense(units,\n",
        "                  activation=activation,\n",
        "                  use_bias=use_bias,\n",
        "                  kernel_initializer=kernel_initializer,\n",
        "                  bias_initializer=bias_initializer,\n",
        "                  kernel_regularizer=kernel_regularizer,\n",
        "                  bias_regularizer=bias_regularizer,\n",
        "                  activity_regularizer=activity_regularizer,\n",
        "                  kernel_constraint=kernel_constraint,\n",
        "                  bias_constraint=bias_constraint,\n",
        "                  trainable=trainable,\n",
        "                  weight_norm=weight_norm,\n",
        "                  mean_only_batch_norm=mean_only_batch_norm,\n",
        "                  name=name,\n",
        "                  dtype=inputs.dtype.base_dtype,\n",
        "                  _scope=name,\n",
        "                  _reuse=reuse)\n",
        "    return layer.apply(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9jAvBmXCKKFO"
      },
      "source": [
        "**Loss of Pi Ensembling Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "18vRXQE5J6IP",
        "colab": {}
      },
      "source": [
        "def pi_model_loss(X_train_labeled, y_train_labeled, X_train_unlabeled,\n",
        "                  pi_model, unsupervised_weight):\n",
        "\n",
        "    z_labeled = pi_model(X_train_labeled)\n",
        "    z_labeled_i = pi_model(X_train_labeled)\n",
        "\n",
        "    z_unlabeled = pi_model(X_train_unlabeled)\n",
        "    z_unlabeled_i = pi_model(X_train_unlabeled)\n",
        "\n",
        "    # Loss = Supervised loss + unsup loss of labeled sample + unsup loss unlabeled sample (Unsupervised Loss)\n",
        "    return tf.losses.softmax_cross_entropy(\n",
        "        y_train_labeled, z_labeled) + unsupervised_weight * (\n",
        "            tf.losses.mean_squared_error(z_labeled, z_labeled_i) +\n",
        "            tf.losses.mean_squared_error(z_unlabeled, z_unlabeled_i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2QKLOS23KYRo"
      },
      "source": [
        "## **Pi Gradient Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "seKFoL2oKdS6",
        "colab": {}
      },
      "source": [
        "def pi_model_gradients(X_train_labeled, y_train_labeled, X_train_unlabeled,\n",
        "                       pi_model, unsupervised_weight):\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = pi_model_loss(X_train_labeled, y_train_labeled, X_train_unlabeled,\n",
        "                                   pi_model, unsupervised_weight)\n",
        "    return loss_value, tape.gradient(loss_value, pi_model.variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BmH1PPemKiTe"
      },
      "source": [
        "## **Ramp-up and -down Function**\n",
        "NB: keep it slow in intial epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UlYt5kXNKuvt",
        "colab": {}
      },
      "source": [
        "def ramp_up_function(epoch, epoch_with_max_rampup=80):\n",
        "\n",
        "    if epoch < epoch_with_max_rampup:\n",
        "        p = max(0.0, float(epoch)) / float(epoch_with_max_rampup)\n",
        "        p = 1.0 - p\n",
        "        return math.exp(-p*p*5.0)\n",
        "    else:\n",
        "        return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "myGAhoNRLMXE",
        "colab": {}
      },
      "source": [
        "def ramp_down_function(epoch, num_epochs):\n",
        "  \n",
        "    epoch_with_max_rampdown = 50\n",
        "\n",
        "    if epoch >= (num_epochs - epoch_with_max_rampdown):\n",
        "        ep = (epoch - (num_epochs - epoch_with_max_rampdown)) * 0.5\n",
        "        return math.exp(-(ep * ep) / epoch_with_max_rampdown)\n",
        "    else:\n",
        "        return 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3jp07fVoLVYs"
      },
      "source": [
        "### **Pi-Model Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "chff2T0qLVG2",
        "colab": {}
      },
      "source": [
        "class PiModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "    \n",
        "        \n",
        "        super(PiModel, self).__init__()\n",
        "        self._conv1a = Conv2D(filters=128, kernel_size=[3, 3],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._conv1b = Conv2D(filters=128, kernel_size=[3, 3],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._conv1c = Conv2D(filters=128, kernel_size=[3, 3],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._pool1 = tf.keras.layers.MaxPool2D(\n",
        "            pool_size=2, strides=2, padding=\"same\")\n",
        "        self._dropout1 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "        self._conv2a = Conv2D(filters=256, kernel_size=[3, 3],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._conv2b = Conv2D(filters=256, kernel_size=[3, 3],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._conv2c = Conv2D(filters=256, kernel_size=[3, 3],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._pool2 = tf.keras.layers.MaxPool2D(\n",
        "            pool_size=2, strides=2, padding=\"same\")\n",
        "        self._dropout2 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "        self._conv3a = Conv2D(filters=512, kernel_size=[3, 3],\n",
        "                                                        padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._conv3b = Conv2D(filters=256, kernel_size=[1, 1],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "        self._conv3c = Conv2D(filters=128, kernel_size=[1, 1],\n",
        "                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                        bias_initializer=tf.keras.initializers.constant(\n",
        "                                                            0.1),\n",
        "                                                        weight_norm=True, mean_only_batch_norm=True)\n",
        "\n",
        "        self._dense = Dense(units=10, activation=tf.nn.softmax,\n",
        "                                                     kernel_initializer=tf.keras.initializers.he_uniform(),\n",
        "                                                     bias_initializer=tf.keras.initializers.constant(\n",
        "                                                         0.1),\n",
        "                                                     weight_norm=True, mean_only_batch_norm=True)\n",
        "\n",
        "    def __aditive_gaussian_noise(self, input, std):\n",
        "\n",
        "        noise = tf.random_normal(shape=tf.shape(\n",
        "            input), mean=0.0, stddev=std, dtype=tf.float32)\n",
        "        return input + noise\n",
        "\n",
        "    def __apply_image_augmentation(self, image):\n",
        "\n",
        "        random_shifts = np.random.randint(-2, 2, (image.numpy().shape[0], 2))\n",
        "        random_transformations = tf.contrib.image.translations_to_projective_transforms(\n",
        "            random_shifts)\n",
        "        image = tf.contrib.image.transform(image, random_transformations, 'NEAREST',\n",
        "                                           output_shape=tf.convert_to_tensor(image.numpy().shape[1:3], dtype=np.int32))\n",
        "        return image\n",
        "\n",
        "    def call(self, input, training=True):\n",
        "\n",
        "        if training:\n",
        "            h = self.__aditive_gaussian_noise(input, 0.15)\n",
        "            h = self.__apply_image_augmentation(h)\n",
        "        else:\n",
        "            h = input\n",
        "\n",
        "        h = self._conv1a(h, training)\n",
        "        h = self._conv1b(h, training)\n",
        "        h = self._conv1c(h, training)\n",
        "        h = self._pool1(h)\n",
        "        h = self._dropout1(h, training=training)\n",
        "\n",
        "        h = self._conv2a(h, training)\n",
        "        h = self._conv2b(h, training)\n",
        "        h = self._conv2c(h, training)\n",
        "        h = self._pool2(h)\n",
        "        h = self._dropout2(h, training=training)\n",
        "\n",
        "        h = self._conv3a(h, training)\n",
        "        h = self._conv3b(h, training)\n",
        "        h = self._conv3c(h, training)\n",
        "\n",
        "        # Average Pooling\n",
        "        h = tf.reduce_mean(h, reduction_indices=[1, 2])\n",
        "        return self._dense(h, training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5_zQHxWfMRo0"
      },
      "source": [
        "## **DataSet Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6jE2JEKqMWEZ",
        "colab": {}
      },
      "source": [
        "from scipy.io import loadmat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U6LkMafFNXXp",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class TfrecordLoader:\n",
        "\n",
        "    def __init__(self, dataset_path, batch_size, epochs, image_size, num_classes, \n",
        "    fraction_of_labeled_per_batch=1.0, fraction_of_unlabeled_per_batch=1.0, shuffle=True):\n",
        "\n",
        "        assert (fraction_of_labeled_per_batch <=1.0 and fraction_of_labeled_per_batch > 0),\"Fraction should be between 0 and 1\"\n",
        "        self._dataset_path = dataset_path\n",
        "        self._labeled_tfrecord_path = dataset_path + '/labeled_train.tfrecords'\n",
        "        self._unlabeled_tfrecord_path = dataset_path + '/unlabeled_train.tfrecords'\n",
        "        self._validation_tfrecord_path = dataset_path + '/validation_set.tfrecords'\n",
        "        self._test_tfrecord_path = dataset_path + '/test_set.tfrecords'\n",
        "        self._batch_size = batch_size\n",
        "        self._epochs = epochs\n",
        "        self._image_size = image_size\n",
        "        self._num_classes = num_classes\n",
        "        self._fraction_of_labeled_per_batch = fraction_of_labeled_per_batch\n",
        "        self._fraction_of_unlabeled_per_batch = fraction_of_unlabeled_per_batch\n",
        "        self._shuffle = shuffle\n",
        "\n",
        "    def load_dataset(self):\n",
        "\n",
        "        def __tfrecord_parser(sample):\n",
        "            \"\"\" Helper parser\n",
        "            \"\"\"\n",
        "            # Image index is needed to keep track of the temporal ensembling past predictions \n",
        "            # without loosing the shuffle batches\n",
        "            keys_to_features = {\n",
        "                'image': tf.FixedLenFeature(\n",
        "                    [self._image_size[0]*self._image_size[1]*self._image_size[2]], tf.float32),\n",
        "                'label': tf.FixedLenFeature([], tf.int64),\n",
        "                'image_index': tf.FixedLenFeature([], tf.int64)\n",
        "            }\n",
        "            parsed_features = tf.parse_single_example(sample, keys_to_features)\n",
        "            image = tf.reshape(parsed_features['image'], self._image_size)\n",
        "            label = tf.one_hot(tf.cast(parsed_features['label'], tf.int64), self._num_classes)\n",
        "            return image, label, tf.cast(parsed_features['image_index'], tf.int64)\n",
        "        \n",
        "        labeled_train_dataset = tf.data.TFRecordDataset([self._labeled_tfrecord_path])\n",
        "        if self._shuffle:\n",
        "            labeled_train_dataset = labeled_train_dataset.shuffle(10000, seed=None, reshuffle_each_iteration=True)\n",
        "            \n",
        "        labeled_train_dataset = labeled_train_dataset.repeat(self._epochs*1000)\n",
        "        labeled_train_dataset = labeled_train_dataset.map(__tfrecord_parser)\n",
        "        if self._fraction_of_labeled_per_batch == 1.0:\n",
        "            labeled_train_dataset = labeled_train_dataset.batch(self._batch_size)\n",
        "        else:\n",
        "            labeled_train_dataset = labeled_train_dataset.batch(\n",
        "                round(self._batch_size*self._fraction_of_labeled_per_batch))\n",
        "        \n",
        "        train_labeled_iterator = labeled_train_dataset.make_one_shot_iterator()\n",
        "        \n",
        "\n",
        "        unlabeled_train_dataset = tf.data.TFRecordDataset([self._unlabeled_tfrecord_path])\n",
        "        if self._shuffle:\n",
        "            unlabeled_train_dataset = unlabeled_train_dataset.shuffle(10000)\n",
        "\n",
        "        unlabeled_train_dataset = unlabeled_train_dataset.repeat(self._epochs)\n",
        "        unlabeled_train_dataset = unlabeled_train_dataset.map(__tfrecord_parser)\n",
        "        if self._fraction_of_labeled_per_batch == 1.0:\n",
        "            unlabeled_train_dataset = unlabeled_train_dataset.batch(self._batch_size)\n",
        "        else:\n",
        "            unlabeled_train_dataset = unlabeled_train_dataset.batch(\n",
        "                round(self._batch_size*self._fraction_of_unlabeled_per_batch))\n",
        "\n",
        "        train_unlabeled_iterator = unlabeled_train_dataset.make_one_shot_iterator()\n",
        "\n",
        "        validation_dataset = tf.data.TFRecordDataset([self._validation_tfrecord_path])\n",
        "        if self._shuffle:\n",
        "            validation_dataset = validation_dataset.shuffle(10000)\n",
        "        validation_dataset = validation_dataset.repeat(self._epochs)\n",
        "        validation_dataset = validation_dataset.map(__tfrecord_parser)\n",
        "        validation_dataset = validation_dataset.batch(self._batch_size)\n",
        "        validation_iterator = validation_dataset.make_one_shot_iterator()\n",
        "\n",
        "        test_dataset = tf.data.TFRecordDataset([self._test_tfrecord_path])\n",
        "        if self._shuffle:\n",
        "            test_dataset = test_dataset.shuffle(10000)\n",
        "        test_dataset = test_dataset.repeat(self._epochs)\n",
        "        test_dataset = test_dataset.map(__tfrecord_parser)\n",
        "        test_dataset = test_dataset.batch(self._batch_size)\n",
        "        test_iterator = test_dataset.make_one_shot_iterator()\n",
        "\n",
        "        return train_labeled_iterator, train_unlabeled_iterator, validation_iterator, test_iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F4imyroLOIft",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "class SvnhLoader:\n",
        "\n",
        "    # Constant attributes\n",
        "    _NUM_TOTAL_SAMPLES = 99289\n",
        "    _TRAIN_URL = 'http://ufldl.stanford.edu/housenumbers/train_32x32.mat'\n",
        "    _TEST_URL = 'http://ufldl.stanford.edu/housenumbers/test_32x32.mat'\n",
        "    _IMAGE_SIZE = [32, 32, 3]\n",
        "    _NUM_CLASSES = 10\n",
        "\n",
        "    def __init__(self, dataset_path, num_train_samples, num_validation_samples,\n",
        "                 num_labeled_samples, random_seed=666):\n",
        "\n",
        "\n",
        "        self._dataset_path = dataset_path\n",
        "        self._num_train_samples = num_train_samples\n",
        "        self._num_test_samples = self._NUM_TOTAL_SAMPLES - self._num_train_samples\n",
        "        self._num_validation_samples = num_validation_samples\n",
        "        self._num_labeled_samples = num_labeled_samples\n",
        "        self._num_unlabeled_train_samples = num_train_samples - \\\n",
        "            num_validation_samples - num_labeled_samples\n",
        "        self._random_seed = random_seed\n",
        "\n",
        "    def __normalize_and_prepare_dataset(self, mat_dataset):\n",
        "\n",
        "        # Convert data to numpy array\n",
        "        X = mat_dataset['X'].astype(np.float64)\n",
        "\n",
        "        # Original dataset comes with wrong order in the dimensions\n",
        "        X = X.transpose((3, 0, 1, 2))\n",
        "\n",
        "        # Convert it to zero mean and unit variance\n",
        "        X -= np.mean(X, axis=(1, 2, 3), keepdims=True)\n",
        "        X /= (np.mean(X ** 2, axis=(1, 2, 3), keepdims=True) ** 0.5)\n",
        "\n",
        "        X = X.reshape([X.shape[0], -1])\n",
        "        y = mat_dataset['y'].flatten().astype(np.int32)\n",
        "        # 0 is label 10\n",
        "        y[y == 10] = 0\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def __download_and_extract_dataset(self):\n",
        "\n",
        "        filepath_train = self._dataset_path + '/train_32x32.mat'\n",
        "        print(filepath_train)\n",
        "        filepath_test = self._dataset_path + '/test_32x32.mat'\n",
        "\n",
        "        def download_progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\r>> Downloading %.1f%%' % (\n",
        "                float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        # Download dataset\n",
        "        urllib.request.urlretrieve(\n",
        "            self._TRAIN_URL, filepath_train, download_progress)\n",
        "        urllib.request.urlretrieve(\n",
        "            self._TEST_URL, filepath_test, download_progress)\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "        # Load resultant mat files\n",
        "        train_data = loadmat(filepath_train)\n",
        "        test_data = loadmat(filepath_test)\n",
        "\n",
        "        # Normalize between 0 and 1\n",
        "        train_X, train_y = self.__normalize_and_prepare_dataset(train_data)\n",
        "        test_X, test_y = self.__normalize_and_prepare_dataset(test_data)\n",
        "\n",
        "        # Remove mat files\n",
        "        os.remove(filepath_train)\n",
        "        os.remove(filepath_test)\n",
        "\n",
        "        return train_X, train_y, test_X, test_y\n",
        "\n",
        "    def __generate_tfrecord(self, images, labels, filename):\n",
        "\n",
        "        # If we are taking care of unlabeled data\n",
        "        if labels == []:\n",
        "            pass\n",
        "        elif images.shape[0] != labels.shape[0]:\n",
        "            raise ValueError(\"Images size %d does not match label size %d.\" %\n",
        "                             (images.shape[0], labels.shape[0]))\n",
        "\n",
        "        print('Writing', filename)\n",
        "\n",
        "        writer = tf.python_io.TFRecordWriter(filename)\n",
        "\n",
        "        # Write each image for the tfrecords file\n",
        "        for index in range(images.shape[0]):\n",
        "            image = images[index].tolist()\n",
        "\n",
        "            # If unlabeled dataset label is -1\n",
        "            if labels == []:\n",
        "                current_label = -1\n",
        "            else:\n",
        "                current_label = int(labels[index])\n",
        "\n",
        "            # Image index is needed to keep track of the temporal ensembling past predictions \n",
        "            # without loosing the shuffle batches\n",
        "            sample = tf.train.Example(features=tf.train.Features(feature={\n",
        "                'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[32])),\n",
        "                'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[32])),\n",
        "                'depth': tf.train.Feature(int64_list=tf.train.Int64List(value=[3])),\n",
        "                'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[current_label])),\n",
        "                'image': tf.train.Feature(float_list=tf.train.FloatList(value=image)),\n",
        "                'image_index': tf.train.Feature(int64_list=tf.train.Int64List(value=[index]))}))\n",
        "            writer.write(sample.SerializeToString())\n",
        "        writer.close()\n",
        "\n",
        "    def download_images_and_generate_tf_record(self):\n",
        "        # Create folder if needed\n",
        "        if not os.path.exists(self._dataset_path):\n",
        "            os.makedirs(self._dataset_path)\n",
        "        else:  # Dataset already loaded\n",
        "            return\n",
        "\n",
        "        # Download and process dataset\n",
        "        train_X, train_y, test_X, test_y = self.__download_and_extract_dataset()\n",
        "\n",
        "        # Use the seed provided\n",
        "        rng = np.random.RandomState(self._random_seed)\n",
        "\n",
        "        # I know I could initalize to zeros to avoid the appends, but it's only\n",
        "        # done once, so let me have it\n",
        "        labeled_train_X = np.empty(shape=(0, 32*32*3))\n",
        "        labeled_train_y = []\n",
        "        unlabeled_train_X = np.empty(shape=(0, 32*32*3))\n",
        "        validation_X = np.empty(shape=(0, 32*32*3))\n",
        "        validation_y = []\n",
        "\n",
        "        # Randomly shuffle the dataset, and have balanced labeled and validation\n",
        "        # datasets (avoid having and unbalenced train set that could hurt the results)\n",
        "        for label in range(10):\n",
        "            label_mask = (train_y == label)\n",
        "            current_label_X = train_X[label_mask]\n",
        "            current_label_y = train_y[label_mask]\n",
        "            current_label_X, current_label_y = rng.permutation(\n",
        "                current_label_X), rng.permutation(current_label_y)\n",
        "            # Take care of the labeled train set\n",
        "            labeled_train_X = np.append(labeled_train_X, current_label_X[:int(\n",
        "                self._num_labeled_samples/self._NUM_CLASSES), :], axis=0)\n",
        "            labeled_train_y = np.append(labeled_train_y, current_label_y[:int(\n",
        "                self._num_labeled_samples/self._NUM_CLASSES)])\n",
        "            current_label_X = current_label_X[int(\n",
        "                self._num_labeled_samples/self._NUM_CLASSES):, :]\n",
        "            current_label_y = current_label_y[int(\n",
        "                self._num_labeled_samples/self._NUM_CLASSES):]\n",
        "            # Now let's take care of validation\n",
        "            validation_X = np.append(validation_X, current_label_X[:int(\n",
        "                self._num_validation_samples/self._NUM_CLASSES)], axis=0)\n",
        "            validation_y = np.append(validation_y, current_label_y[:int(\n",
        "                self._num_validation_samples/self._NUM_CLASSES)])\n",
        "            current_label_X = current_label_X[int(\n",
        "                self._num_validation_samples/self._NUM_CLASSES):, :]\n",
        "            current_label_y = current_label_y[int(\n",
        "                self._num_validation_samples/self._NUM_CLASSES):]\n",
        "            # The rest goes to Unlabeled train\n",
        "            unlabeled_train_X = np.append(\n",
        "                unlabeled_train_X, current_label_X, axis=0)\n",
        "\n",
        "        # Print final set shapes\n",
        "        print(\"Labeled train shape: \", labeled_train_X.shape)\n",
        "        print(\"Unlabeled train shape: \", unlabeled_train_X.shape)\n",
        "        print(\"Validation shape: \", validation_X.shape)\n",
        "        print(\"Test shape: \", test_X.shape)\n",
        "\n",
        "        # Write tfrecords to disk\n",
        "        self.__generate_tfrecord(labeled_train_X, labeled_train_y, os.path.join(\n",
        "            self._dataset_path, 'labeled_train.tfrecords'))\n",
        "\n",
        "        self.__generate_tfrecord(unlabeled_train_X, [], os.path.join(\n",
        "            self._dataset_path, 'unlabeled_train.tfrecords'))\n",
        "\n",
        "        self.__generate_tfrecord(validation_X, validation_y, os.path.join(\n",
        "            self._dataset_path, 'validation_set.tfrecords'))\n",
        "\n",
        "        self.__generate_tfrecord(test_X, test_y, os.path.join(\n",
        "            self._dataset_path, 'test_set.tfrecords'))\n",
        "\n",
        "    def load_dataset(self, batch_size, epochs, fraction_of_labeled_per_batch=1.0,\n",
        "                     fraction_of_unlabeled_per_batch=1.0, shuffle=True):\n",
        "\n",
        "        tfrecord_loader = TfrecordLoader(\n",
        "            './data', batch_size, epochs, self._IMAGE_SIZE, self._NUM_CLASSES,\n",
        "            fraction_of_labeled_per_batch, fraction_of_unlabeled_per_batch, shuffle)\n",
        "        return tfrecord_loader.load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gONesMVXMB-e"
      },
      "source": [
        "## **Main Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "62MN7tgnMHKz",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import tensorflow.contrib.eager as tfe\n",
        "tf.enable_eager_execution()\n",
        "import os\n",
        "import urllib\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vj7wP9KNMEDF",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Constants variables\n",
        "    NUM_TRAIN_SAMPLES = 73257\n",
        "    NUM_TEST_SAMPLES = 26032\n",
        "\n",
        "    # Editable variables\n",
        "    num_labeled_samples = 1000\n",
        "    num_validation_samples = 200\n",
        "    num_train_unlabeled_samples = NUM_TRAIN_SAMPLES - \\\n",
        "        num_labeled_samples - num_validation_samples\n",
        "    batch_size = 25\n",
        "    epochs = 300\n",
        "    max_learning_rate = 0.001\n",
        "    initial_beta1 = 0.9\n",
        "    final_beta1 = 0.5\n",
        "    checkpoint_directory = './checkpoints/PiModel'\n",
        "    tensorboard_logs_directory = './logs/PiModel'\n",
        "\n",
        "    # Assign it as tfe.variable since we will change it across epochs\n",
        "    learning_rate = tfe.Variable(max_learning_rate)\n",
        "    beta_1 = tfe.Variable(initial_beta1)\n",
        "\n",
        "    # Download and Save Dataset in Tfrecords/content/data\n",
        "    loader = SvnhLoader('/content/data', NUM_TRAIN_SAMPLES,\n",
        "                        num_validation_samples, num_labeled_samples)\n",
        "    loader.download_images_and_generate_tf_record()\n",
        "    # Generate data loaders\n",
        "    train_labeled_iterator, train_unlabeled_iterator, validation_iterator, test_iterator = loader.load_dataset(\n",
        "        batch_size, epochs)\n",
        "\n",
        "    batches_per_epoch = int(num_labeled_samples/batch_size)\n",
        "    batches_per_epoch_val = int(num_validation_samples / batch_size)\n",
        "\n",
        "    model = PiModel()\n",
        "    optimizer = tf.train.AdamOptimizer(\n",
        "        learning_rate=learning_rate, beta1=beta_1, beta2=0.990)\n",
        "    max_unsupervised_weight = 100 * num_labeled_samples / \\\n",
        "        (NUM_TRAIN_SAMPLES - num_validation_samples)\n",
        "    \n",
        "    best_val_accuracy = 0\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    writer = tf.contrib.summary.create_file_writer(tensorboard_logs_directory)\n",
        "    writer.set_as_default()\n",
        "\n",
        "    for epoch in trange(epochs):\n",
        "\n",
        "        rampdown_value = ramp_down_function(epoch, epochs)\n",
        "        rampup_value = ramp_up_function(epoch)\n",
        "\n",
        "        if epoch == 0:\n",
        "            unsupervised_weight = 0\n",
        "        else:\n",
        "            unsupervised_weight = max_unsupervised_weight * \\\n",
        "                rampup_value\n",
        "\n",
        "        learning_rate.assign(rampup_value * rampdown_value * max_learning_rate)\n",
        "        beta_1.assign(rampdown_value * initial_beta1 +\n",
        "                      (1.0 - rampdown_value) * final_beta1)\n",
        "\n",
        "        epoch_loss_avg = tfe.metrics.Mean()\n",
        "        epoch_accuracy = tfe.metrics.Accuracy()\n",
        "        epoch_loss_avg_val = tfe.metrics.Mean()\n",
        "        epoch_accuracy_val = tfe.metrics.Accuracy()\n",
        "        for batch_nr in trange(batches_per_epoch):\n",
        "            X_labeled_train, y_labeled_train, _ = train_labeled_iterator.get_next()\n",
        "            X_unlabeled_train, _, _ = train_unlabeled_iterator.get_next()\n",
        "\n",
        "            loss_val, grads = pi_model_gradients(X_labeled_train, y_labeled_train, X_unlabeled_train,\n",
        "                                                 model, unsupervised_weight)\n",
        "            \n",
        "            optimizer.apply_gradients(zip(grads, model.variables),\n",
        "                                      global_step=global_step)\n",
        "            epoch_loss_avg(loss_val)\n",
        "            epoch_accuracy(\n",
        "                tf.argmax(model(X_labeled_train), 1), tf.argmax(y_labeled_train, 1))\n",
        "            if (batch_nr == batches_per_epoch - 1):\n",
        "                for batch_val_nr in trange(batches_per_epoch_val):\n",
        "                    X_val, y_val, _ = validation_iterator.get_next()\n",
        "                    y_val_predictions = model(X_val, training=False)\n",
        "\n",
        "                    epoch_loss_avg_val(tf.losses.softmax_cross_entropy(\n",
        "                        y_val, y_val_predictions))\n",
        "                    epoch_accuracy_val(\n",
        "                        tf.argmax(y_val_predictions, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "        print(\"Epoch {:03d}/{:03d}: Train Loss: {:9.7f}, Train Accuracy: {:02.6%}, Validation Loss: {:9.7f}, \"\n",
        "              \"Validation Accuracy: {:02.6%}, lr={:.9f}, unsupervised weight={:5.3f}, beta1={:.9f}\".format(epoch+1,\n",
        "                                                                                                           epochs,\n",
        "                                                                                                           epoch_loss_avg.result(),\n",
        "                                                                                                           epoch_accuracy.result(),\n",
        "                                                                                                           epoch_loss_avg_val.result(),\n",
        "                                                                                                           epoch_accuracy_val.result(),\n",
        "                                                                                                           learning_rate.numpy(),\n",
        "                                                                                                           unsupervised_weight,\n",
        "                                                                                                           beta_1.numpy()))\n",
        "\n",
        "        # If the accuracy of validation improves save a checkpoint Best 85%\n",
        "        if best_val_accuracy < epoch_accuracy_val.result():\n",
        "            best_val_accuracy = epoch_accuracy_val.result()\n",
        "            checkpoint = tfe.Checkpoint(optimizer=optimizer,\n",
        "                                        model=model,\n",
        "                                        optimizer_step=global_step)\n",
        "            checkpoint.save(file_prefix=checkpoint_directory)\n",
        "\n",
        "        # Record summaries\n",
        "        with tf.contrib.summary.record_summaries_every_n_global_steps(1):\n",
        "            tf.contrib.summary.scalar('Train Loss', epoch_loss_avg.result())\n",
        "            tf.contrib.summary.scalar(\n",
        "                'Train Accuracy', epoch_accuracy.result())\n",
        "            tf.contrib.summary.scalar(\n",
        "                'Validation Loss', epoch_loss_avg_val.result())\n",
        "            tf.contrib.summary.scalar(\n",
        "                'Validation Accuracy', epoch_accuracy_val.result())\n",
        "            tf.contrib.summary.scalar(\n",
        "                'Unsupervised Weight', unsupervised_weight)\n",
        "            tf.contrib.summary.scalar('Learning Rate', learning_rate.numpy())\n",
        "            tf.contrib.summary.scalar('Ramp Up Function', rampup_value)\n",
        "            tf.contrib.summary.scalar('Ramp Down Function', rampdown_value)\n",
        "            \n",
        "\n",
        "    print('\\nTrain Ended! Best Validation accuracy = {}\\n'.format(best_val_accuracy))\n",
        "\n",
        "    # Load the best model\n",
        "    root = tfe.Checkpoint(optimizer=optimizer,\n",
        "                          model=model,\n",
        "                          optimizer_step=tf.train.get_or_create_global_step())\n",
        "    root.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
        "\n",
        "    # Evaluate on the final test set\n",
        "    num_test_batches = math.ceil(NUM_TEST_SAMPLES/batch_size)\n",
        "    test_accuracy = tfe.metrics.Accuracy()\n",
        "    for test_batch in range(num_test_batches):\n",
        "        X_test, y_test, _ = test_iterator.get_next()\n",
        "        y_test_predictions = model(X_test, training=False)\n",
        "        test_accuracy(tf.argmax(y_test_predictions, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "    print(\"Final Test Accuracy: {:.6%}\".format(test_accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NRYL9jclQHen"
      },
      "source": [
        "## **Run the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zhwowx_0QG-M",
        "outputId": "ab8ab4a0-bb02-4b46-e9f3-6172d11bcc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "1197b01ceb9f463db3a49876ced36f4c",
            "ea61a0eaa07541c1a5e5f11e92685999",
            "5faf311b82d04dbb8d2d54e35041f084",
            "5d4e1c2f3c7c4576a568912fbc367b97",
            "4dc4c560c5d245ac806ef4d49f3b587f",
            "18e07c31fa714d9cb1dd367340f7dcf9",
            "348c8eeda4bf4252bb6cd288e35d91bc",
            "704ea2d76af04886953e0160229879a4",
            "80eb3e6a517144cbbc1ab1666bfef08e",
            "45602955c4904e2588d7d6564456b98f",
            "83c227f38a214ac6af042cd4b5adfd9a",
            "b2df7d50ab53451595d4d80f73615039",
            "404cc24a610447fa8f0e66cdd6fb0139",
            "1ffacc387d7c4b2796c3c5c2b7b9da1e",
            "2ce562d92dc94732892d32f95d9afa6b",
            "b7873bb4310c49288119e12edb74358f"
          ]
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data/train_32x32.mat\n",
            ">> Downloading 100.0%\n",
            "\n",
            "Labeled train shape:  (1000, 3072)\n",
            "Unlabeled train shape:  (72057, 3072)\n",
            "Validation shape:  (200, 3072)\n",
            "Test shape:  (26032, 3072)\n",
            "Writing /content/data/labeled_train.tfrecords\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:93: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing /content/data/unlabeled_train.tfrecords\n",
            "Writing /content/data/validation_set.tfrecords\n",
            "Writing /content/data/test_set.tfrecords\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1197b01ceb9f463db3a49876ced36f4c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80eb3e6a517144cbbc1ab1666bfef08e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=40.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3A07jGpHEGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}